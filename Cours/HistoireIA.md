# Historique de l’Intelligence Artificielle en médecine et biologie (Alexandre Godmer, Guillaume Bachelot)

**Source : Thèse d'université, Dr Alexandre Godmer, Sorbonne-Université, ED515, "Techniques protéomiques et intelligence artificielle : des solutions pour l’identification et la détection de la résistance en microbiologie ?"**

L’histoire de l’IA est vaste mais quelques dates clés permettent de mieux comprendre comment cette technologie a pris une place importante dans de nombreux domaines [1] [2]. Le terme « Intelligence Artificielle » (IA) est apparu pour la première fois en 1956 lors d’une conférence à Dartmouth durant laquelle les participants dont John McCarthy Marvin Minsky et Claude Shannon ont défini les bases théoriques de l’IA. Bien que cette date soit considérée comme le point de départ de l’IA plusieurs travaux concernant ce domaine avaient déjà été développés comme la création d’un modèle informatique simplifié de neurone biologique par McCulloch-Pitts en 1943 et le « test de Turing » qui permet de déterminer la capacité d'une machine à reproduire les comportements d'un être humain en termes d’intelligences mis au point par Alan Turing en 1950 [3] [4].

Entre 1956 et 1970 l’IA était en pleine expansion c’est pourquoi cette période est souvent considérée comme « l’âge d’or » de l’IA bien qu’aucune application pratique n’ait été mise au point. Cette époque a surtout été riche conceptuellement puisque beaucoup de modèles ont été développés durant ces années. Par exemple en 1958 Frank Rosenblatt a mis au point « le perceptron » un modèle de réseau neuronal artificiel simple et monocouche capable d'apprendre à reconnaître des motifs [5]. Entre 1960 et 1970 de nombreux langages de programmation dédiés à l'IA et de systèmes experts ont été développés. Par exemple en 1966 ELIZA (en référence au personnage d’Eliza Doolittle dans le drame de Bernard Shaw intitulé Pygmalion) développé par Joseph Weizenbaum est le premier programme de chatbot utilisant un programme de traitement du langage naturel ou « Natural Language Processing » (NLP) capable de simuler une conversation thérapeutique entre un psychothérapeute et un patient. C’est ce programme qui a ouvert le domaine de l’interface homme-machine.

La période entre 1971 et 1980 est considérée comme « la première hibernation de l’IA » (IA Winter) en raison d'un déclin de l'intérêt et d'une réduction des financements dans ce domaine [6]. Ce déclin était dû à la déception face au petit nombre d’applications développées au regard des attentes très à vrai dire trop élevées soulevées par l’IA. Divers obstacles notamment les capacités de calcul limitées des ordinateurs de l'époque expliquaient les limites de développement d’applications de l'IA. Cependant certains programmes majeurs ont vu le jour durant cette période. En 1971 Saul Amarel a développé « The Research Resource on Computers in Biomedicine » à l'Université Rutgers dans le but de créer un système informatique visant à connecter des chercheurs et des praticiens issus de la communauté scientifique et médicale de diverses institutions. L'objectif de ce projet était d'explorer les applications potentielles de l’IA pour améliorer la recherche et la pratique médicale. Cette innovation a joué un rôle important pour le développement d’outils appliqués à la médecine. En 1972 le système expert MYCIN (en référence au suffixe « mycin » rencontré dans la classe d’antibiotiques des aminoglycosides) est lancé à l'université de Stanford pour aider au diagnostic et au traitement des infections bactériennes. Ce système expert permettait de proposer des recommandations médicales concernant le choix de l’antibiotique en fonction de la bactérie et du poids des patients [7]. En 1978 le modèle CASNET (Causal-Associative Network) développé par la Rutgers University a constitué l’un des premiers systèmes d’aide à la prise de décision médicale à l’aide d’un algorithme informatique pour la détection du glaucome.

Entre 1980 et 1987 l’IA a connu un second souffle. En 1986 un système d'aide au diagnostic médical nommé « DXplain » (pour « Diagnosis Explain ») développé par la Harvard Medical School a été mis au point pour analyser les données clinico-biologiques comme des symptômes des résultats de tests médicaux et des antécédents pour générer une liste de maladies ou de diagnostics possibles. Cependant aucun de ces systèmes cités ci-dessus n’était suffisamment robuste pour être largement utilisé par la communauté médicale. La prise de conscience que l’IA a un champ des possibles limité a entraîné un second hiver de l’IA de 1987 jusqu’au milieu des années 1990.

A la fin des années 1990 l’IA a connu un regain d’intérêt du fait de l’augmentation de la puissance des ordinateurs [8]. En 1997 IBM (International Business Machines Corporation) a créé un programme nommé Deep Blue fonctionnant sur un superordinateur connu pour avoir battu aux échecs le champion du monde Garry Kasparov [9].

Le début des années 2000 est marqué par l’avènement des techniques d’apprentissage profond (ou Deep Learning DL). Ces algorithmes dont l’unité de base est le neurone artificiel peuvent être organisés en réseaux sous forme de couches et former différentes architectures. Déjà en 1986 Geoffrey Hinton David Rumelhart et Ronald Williams avaient publié un travail sur l'apprentissage de réseaux de neurones profonds avec la technique de la rétropropagation du gradient. Cet algorithme avait été un tournant majeur dans le domaine de l’apprentissage profond [10]. Cependant dans les années 1980 et 1990 les ressources informatiques étaient trop limitées et la quantité nécessaire de données pour entraîner les réseaux de neurones n’était pas disponible.

En 2006 IBM a démarré le programme Watson qui avait pour objectif de répondre à des questions en langage naturel. En 2017 ce programme est parvenu à identifier de nouvelles protéines de liaison à l'ARN altérées dans la sclérose latérale amyotrophique à partir d’informations textuelles provenant de publications scientifiques [11]. De nombreux projets appliqués à la médecine ou la biologie ont vu le jour depuis la fin des années 2010.

L’un des tournant essentiel de l’IA est l’utilisation d’une architecture de réseaux de neurones spécifique connue sous le nom de réseaux de neurones convolutifs (ou Convolutional Neural Networks CNN) appliquée à la reconnaissance d’images et à la vision par ordinateur. A ce sujet c’est en en 2012 que le réseau AlexNet a obtenu des résultats sans précédents lors de la compétition ImageNet Large Scale Visual Recognition Challenge (ILSVRC) pour la classification d’images [12]. L’utilisation des techniques de DL a permis par exemple la création de systèmes performants dans de nombreux domaines comme le diagnostic de la rétinopathie diabétique le diagnostic des cancers de la peau la prédiction du risque cardiovasculaire [13] [14] [15]. Les CNNs suscitent un intérêt particulier dans le domaine du diagnostic médical car ils sont capables de saisir l'information texturale à partir d’images radiologiques [16]. A ce sujet les CNNs sont particulièrement utilisés dans le domaine de la recherche médicale pour l’étude d’images médicales [17]. En 2023 une soixantaine d’algorithmes ont obtenu la 510(k) premarket notification de la FDA (Food and Drug Administration) indispensable pour commercialiser un dispositif médical aux Etats-Unis (https://medicalfuturist.com/fda-approved-ai-based-algorithms/) [18]. Plus récemment le DL s’est avéré utile pour le diagnostic de la Covid-19 à partir d’images radiologiques [19]. Plusieurs autres exemples récents dans le domaine de la microbiologie peuvent aussi être cités. En 2021 des chercheurs ont mis en avant l’utilisation du DL pour prédire l’activité des peptides antimicrobiens [20]. D’autres travaux se sont intéressés aux techniques de DL pour l'analyse d'images à la recherche de micro-organismes provenant de frottis analysés au microscope [21] [22]. A ce propos des travaux ont montré qu’il était possible de classer des images de culture bactérienne (positives ou négatives) à l’aide des techniques de DL. Certains de ces outils comme le système PhenoMATRIX™ (bioMérieux® Craponne France) sont déjà utilisés en routine au laboratoire de microbiologie et intégrés dans les systèmes de chaînes d’automatisation qui sont de plus en plus répandues [23] [24]. Pour citer un autre exemple un système d’analyse numérique portable a aussi été mis au point pour la détection du paludisme à partir de frottis dans le but d’améliorer le diagnostic dans les régions où l’accès aux équipements de laboratoire est limité [25].

## Références

[1] Callier, P., & Sandel, O. (2021). De l’intelligence artificielle à son application en médecine. Actualités Pharmaceutiques, 60(611), 18-20. doi: 10.1016/j.actpha.2021.10.005.

[2] Kaul, V., Enslin, S., & Gross, S. A. (2020). History of artificial intelligence in medicine. Gastrointestinal Endoscopy, 92(4), 807-812. doi: 10.1016/j.gie.2020.06.040.

[3] Mcculloch, W. S., & Pitts, W. (1943). A Logical Calculus of the Ideas Immanent in Nervous Activity.

[4] Turing, A. M. (1950). Computing Machinery and Intelligence. Mind, LIX(236), 433-460. doi: 10.1093/mind/LIX.236.433.

[5] Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386-408. doi: 10.1037/h0042519.

[6] Greenhill, A. T., & Edmunds, B. R. (2020). A primer of artificial intelligence in medicine. Techniques and Innovations in Gastrointestinal Endoscopy, 22(2), 85-89. doi: 10.1016/j.tgie.2019.150642.

[7] Shortliffe, E. H., & Buchanan, B. G. (1975). A model of inexact reasoning in medicine. Mathematical Biosciences, 23(3), 351-379. doi: 10.1016/0025-5564(75)90047-4.

[8] Haug, C. J., & Drazen, J. M. (2023). Artificial Intelligence and Machine Learning in Clinical Medicine. New England Journal of Medicine, 388(13), 1201-1208. doi: 10.1056/NEJMra2302038.

[9] McGrew, T. (1997). Collaborative intelligence. The Internet Chess Club on game 2 of Kasparov vs. Deep Blue. IEEE Internet Computing, 1(3), 38-42. doi: 10.1109/4236.589193.

[10] Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536. doi: 10.1038/323533a0.

[11] Bakkar, N., et al. (2018). Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis. Acta Neuropathologica, 135(2), 227-247. doi: 10.1007/s00401-017-1785-8.

[12] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems. Curran Associates, Inc. Retrieved September 5, 2023, from https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html

[13] Gargeya, R., & Leng, T. (2017). Automated Identification of Diabetic Retinopathy Using Deep Learning. Ophthalmology, 124(7), 962-969. doi: 10.1016/j.ophtha.2017.02.008.

[14] Esteva, A., et al. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118. doi: 10.1038/nature21056.

[15] Weng, S. F., Reps, J., Kai, J., Garibaldi, J. M., & Qureshi, N. (2017). Can machine-learning improve cardiovascular risk prediction using routine clinical data? 
PLoS One, 12(4), e0174944. doi: 10.1371/journal.pone.0174944.

[16] Parekh, V. S., & Jacobs, M. A. (2019). Deep learning and radiomics in precision medicine. Expert Review of Precision Medicine and Drug Development, 4(2), 59-72. doi: 10.1080/23808993.2019.1585805.

[17] Bakator, M., & Radosav, D. (2018). Deep Learning and Medical Diagnosis: A Review of Literature. Multimodal Technologies and Interaction, 2(3), 47. doi: 10.3390/mti2030047.

[18] Benjamens, S., Dhunnoo, P., & Meskó, B. (2020). The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database. NPJ Digital Medicine, 3, 118. doi: 10.1038/s41746-020-00324-0.

[19] Apostolopoulos, I. D., & Mpesiana, T. A. (2020). Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks. Physics and Engineering Sciences in Medicine, 43(2), 635-640. doi: 10.1007/s13246-020-00865-4.

[20] Hernández Medina, R., et al. (2022). Machine learning and deep learning applications in microbiome research. ISME COMMUNICATIONS, 2(1), 182. doi: 10.1038/s43705-022-00182-9.

[21] Lin, C. (2023). Deep learning networks for microbiology images recognition. In Third International Conference on Artificial Intelligence and Computer Engineering (ICAICE 2022). SPIE, 456-463. doi: 10.1117/12.2671439.

[22] Zhang, Y., Jiang, H., Ye, T., & Juhas, M. (2021). Deep Learning for Imaging and Detection of Microorganisms. Trends in Microbiology, 29(7), 569-572. doi: 10.1016/j.tim.2021.01.006.

[23] Faron, M. L., Buchan, B. W., Relich, R. F., Clark, J., & Ledeboer, N. A. (2020). Evaluation of the WASPLab Segregation Software To Automatically Analyze Urine Cultures Using Routine Blood and MacConkey Agars. Journal of Clinical Microbiology, 58(4), e01683-19. doi: 10.1128/JCM.01683-19.

[24] Glasson, J., et al. (2017). Multicenter Evaluation of an Image Analysis Device (APAS): Comparison Between Digital Image and Traditional Plate Reading Using Urine Cultures. Annals of Laboratory Medicine, 37(6), 499-504. doi: 10.3343/alm.2017.37.6.499.

[25] Horning, M. P., et al. (2021). Performance of a fully‐automated system on a WHO malaria microscopy evaluation slide set. Malaria Journal, 20(1), 110. doi: 10.1186/s12936-021-03631-3.
